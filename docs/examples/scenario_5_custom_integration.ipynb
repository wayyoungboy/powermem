{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 5: Custom Integration\n",
    "\n",
    "This scenario demonstrates how to integrate powermem with custom systems, implement custom providers, and extend functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Completed previous scenarios\n",
    "- Understanding of Python classes and interfaces\n",
    "- Basic knowledge of LLM/embedding providers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Custom Integration\n",
    "\n",
    "powermem is designed to be extensible:\n",
    "- Custom LLM providers\n",
    "- Custom embedding providers\n",
    "- Custom storage backends\n",
    "- Custom intelligence plugins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provider and Factory Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "from powermem.integrations.llm.config.base import BaseLLMConfig\n",
    "from powermem.integrations.embeddings.config.base import BaseEmbedderConfig\n",
    "from powermem.storage.config.base import BaseVectorStoreConfig\n",
    "\n",
    "class CustomLLMConfig(BaseLLMConfig):\n",
    "    base_url: str | None = Field(default=None)\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "\n",
    "class CustomEmbedderConfig(BaseEmbedderConfig):\n",
    "    dims: int = Field(default=768)\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "\n",
    "class CustomVectorStoreConfig(BaseVectorStoreConfig):\n",
    "    connection_string: str = Field(default='')\n",
    "    collection_name: str = Field(default='memories')\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "\n",
    "from powermem.integrations.llm.factory import LLMFactory\n",
    "from powermem.integrations.embeddings.factory import EmbedderFactory\n",
    "from powermem.storage.factory import VectorStoreFactory\n",
    "\n",
    "LLMFactory.provider_to_class.update({\n",
    "    \"custom\": (\"powermem.integrations.llm.custom.CustomLLM\", CustomLLMConfig),\n",
    "})\n",
    "\n",
    "EmbedderFactory.provider_to_class.update({\n",
    "    \"custom\": \"powermem.integrations.embeddings.custom.CustomEmbedder\"\n",
    "})\n",
    "\n",
    "VectorStoreFactory.provider_to_class.update({\n",
    "     \"custom\": \"powermem.storage.custom.custom_integration_example.CustomVectorStore\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Custom LLM Provider\n",
    "\n",
    "Implement a custom LLM provider:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from powermem.integrations.llm.base import LLMBase\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class CustomLLM(LLMBase):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.api_key = config.get('api_key', '')\n",
    "        self.model = config.get('model', 'default')\n",
    "        self.base_url = config.get('base_url', 'https://api.example.com')\n",
    "    \n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Generate text using custom LLM\"\"\"\n",
    "        # Your custom LLM implementation\n",
    "        # This is a mock example\n",
    "        return f\"Response to: {prompt}\"\n",
    "    \n",
    "    def extract_facts(self, messages: List[Dict[str, str]]) -> List[str]:\n",
    "        \"\"\"Extract facts from messages\"\"\"\n",
    "        # Custom fact extraction logic\n",
    "        facts = []\n",
    "        for msg in messages:\n",
    "            if msg.get('role') == 'user':\n",
    "                content = msg.get('content', '')\n",
    "                # Simple extraction (replace with your logic)\n",
    "                if 'name is' in content.lower():\n",
    "                    facts.append(f\"Name: {content.split('name is')[1].strip()}\")\n",
    "        return facts\n",
    "\n",
    "# Register custom LLM\n",
    "from powermem.integrations.llm.factory import LLMFactory\n",
    "\n",
    "# Register your custom provider\n",
    "LLMFactory.register('custom', CustomLLM)\n",
    "\n",
    "# Use custom LLM\n",
    "config = {\n",
    "    'llm': {\n",
    "        'provider': 'custom',\n",
    "        'config': {\n",
    "            'api_key': 'your_key',\n",
    "            'model': 'your_model',\n",
    "            'base_url': 'https://api.example.com'\n",
    "        }\n",
    "    },\n",
    "    'embedder': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'text-embedding-v4'}\n",
    "    },\n",
    "    'vector_store': {\n",
    "        'provider': 'sqlite',\n",
    "        'config': {'path': './memories.db'}\n",
    "    }\n",
    "}\n",
    "\n",
    "from powermem import Memory\n",
    "memory = Memory(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Custom Embedding Provider\n",
    "\n",
    "Implement a custom embedding provider:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from powermem.integrations.embeddings.base import EmbedderBase\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "class CustomEmbedder(EmbedderBase):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.api_key = config.get('api_key', '')\n",
    "        self.model = config.get('model', 'default')\n",
    "        self.dims = config.get('dims', 768)\n",
    "    \n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for text\"\"\"\n",
    "        # Your custom embedding implementation\n",
    "        # This is a mock example\n",
    "        # In real implementation, call your embedding API\n",
    "        return np.random.rand(self.dims).tolist()\n",
    "    \n",
    "    def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for batch of texts\"\"\"\n",
    "        return [self.embed(text) for text in texts]\n",
    "\n",
    "# Register custom embedder\n",
    "from powermem.integrations.embeddings.factory import EmbedderFactory\n",
    "\n",
    "EmbedderFactory.register('custom', CustomEmbedder)\n",
    "\n",
    "# Use custom embedder\n",
    "config = {\n",
    "    'llm': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'qwen-plus'}\n",
    "    },\n",
    "    'embedder': {\n",
    "        'provider': 'custom',\n",
    "        'config': {\n",
    "            'api_key': 'your_key',\n",
    "            'model': 'your_model',\n",
    "            'dims': 768\n",
    "        }\n",
    "    },\n",
    "    'vector_store': {\n",
    "        'provider': 'sqlite',\n",
    "        'config': {'path': './memories.db'}\n",
    "    }\n",
    "}\n",
    "\n",
    "memory = Memory(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Custom Storage Backend\n",
    "\n",
    "Implement a custom storage backend:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from powermem.storage.base import VectorStoreBase\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class CustomVectorStore(VectorStoreBase):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.connection_string = config.get('connection_string', '')\n",
    "        self.collection_name = config.get('collection_name', 'memories')\n",
    "        # Initialize your custom storage connection\n",
    "        self._init_connection()\n",
    "    \n",
    "    def _init_connection(self):\n",
    "        \"\"\"Initialize connection to your storage\"\"\"\n",
    "        # Your connection initialization logic\n",
    "        pass\n",
    "    \n",
    "    def add(self, memory: str, embedding: List[float], metadata: Dict[str, Any]) -> str:\n",
    "        \"\"\"Add memory to storage\"\"\"\n",
    "        memory_id = f\"mem_{hash(memory)}\"\n",
    "        # Your storage implementation\n",
    "        # Store memory, embedding, and metadata\n",
    "        return memory_id\n",
    "    \n",
    "    def search(self, query_embedding: List[float], limit: int = 10, \n",
    "               filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search similar memories\"\"\"\n",
    "        # Your search implementation\n",
    "        # Return list of memories with scores\n",
    "        return []\n",
    "    \n",
    "    def get(self, memory_id: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get memory by ID\"\"\"\n",
    "        # Your retrieval implementation\n",
    "        return None\n",
    "    \n",
    "    def update(self, memory_id: str, memory: Optional[str] = None,\n",
    "               metadata: Optional[Dict[str, Any]] = None) -> bool:\n",
    "        \"\"\"Update memory\"\"\"\n",
    "        # Your update implementation\n",
    "        return True\n",
    "    \n",
    "    def delete(self, memory_id: str) -> bool:\n",
    "        \"\"\"Delete memory\"\"\"\n",
    "        # Your deletion implementation\n",
    "        return True\n",
    "    \n",
    "    def delete_all(self, filters: Optional[Dict[str, Any]] = None) -> int:\n",
    "        \"\"\"Delete all matching memories\"\"\"\n",
    "        # Your bulk deletion implementation\n",
    "        return 0\n",
    "\n",
    "# Register custom storage\n",
    "from powermem.storage.factory import VectorStoreFactory\n",
    "\n",
    "VectorStoreFactory.register('custom', CustomVectorStore)\n",
    "\n",
    "# Use custom storage\n",
    "config = {\n",
    "    'llm': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'qwen-plus'}\n",
    "    },\n",
    "    'embedder': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'text-embedding-v4'}\n",
    "    },\n",
    "    'vector_store': {\n",
    "        'provider': 'custom',\n",
    "        'config': {\n",
    "            'connection_string': 'your_connection_string',\n",
    "            'collection_name': 'memories'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "memory = Memory(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: LangChain Integration\n",
    "\n",
    "Integrate powermem with LangChain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_integration.py\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from powermem import Memory, auto_config\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Create powermem instance\n",
    "config = auto_config()\n",
    "powermem = Memory(config=config)\n",
    "\n",
    "class PowermemLangChainMemory:\n",
    "    \"\"\"Custom memory class for LangChain 1.1.0+ integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, powermem_instance, user_id: str):\n",
    "        self.powermem = powermem_instance\n",
    "        self.user_id = user_id\n",
    "        self.messages: List[BaseMessage] = []\n",
    "    \n",
    "    def add_message(self, message: BaseMessage):\n",
    "        \"\"\"Add a message to conversation history.\"\"\"\n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def get_messages(self) -> List[BaseMessage]:\n",
    "        \"\"\"Get all conversation messages.\"\"\"\n",
    "        return self.messages\n",
    "    \n",
    "    def save_to_powermem(self, user_input: str, assistant_output: str):\n",
    "        \"\"\"Save conversation to powermem with intelligent processing.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_output}\n",
    "        ]\n",
    "        self.powermem.add(\n",
    "            messages=messages,\n",
    "            user_id=self.user_id,\n",
    "            infer=True  # Enable intelligent fact extraction\n",
    "        )\n",
    "    \n",
    "    def get_context(self, query: str) -> str:\n",
    "        \"\"\"Load relevant memories from powermem.\"\"\"\n",
    "        results = self.powermem.search(\n",
    "            query=query,\n",
    "            user_id=self.user_id,\n",
    "            limit=5\n",
    "        )\n",
    "        memories = results.get('results', [])\n",
    "        if memories:\n",
    "            return \"\\n\".join([mem.get('memory', '') for mem in memories])\n",
    "        return \"No previous context found.\"\n",
    "\n",
    "# Usage with LangChain 1.1.0+\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "memory = PowermemLangChainMemory(powermem, user_id=\"user123\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use the following context to provide personalized responses.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain with context retrieval\n",
    "def format_messages(input_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Retrieve context and format messages.\"\"\"\n",
    "    user_input = input_dict.get(\"input\", \"\")\n",
    "    context = memory.get_context(user_input)\n",
    "    messages = memory.get_messages()\n",
    "    \n",
    "    formatted_history = []\n",
    "    if context and \"No previous\" not in context:\n",
    "        formatted_history.append((\"system\", f\"Context: {context}\"))\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            formatted_history.append((\"human\", msg.content))\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            formatted_history.append((\"assistant\", msg.content))\n",
    "    \n",
    "    return {\"history\": formatted_history, \"input\": user_input}\n",
    "\n",
    "# Create the chain using LCEL (LangChain Expression Language)\n",
    "chain = (\n",
    "    RunnableLambda(format_messages)\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Use the chain\n",
    "user_input = \"Hello, I'm Alice\"\n",
    "memory.add_message(HumanMessage(content=user_input))\n",
    "\n",
    "response = chain.invoke({\"input\": user_input})\n",
    "response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "memory.add_message(AIMessage(content=response_text))\n",
    "memory.save_to_powermem(user_input, response_text)\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: LangGraph Integration\n",
    "\n",
    "Integrate powermem with LangGraph 1.0+ for stateful workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langgraph_integration.py\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from powermem import Memory, auto_config\n",
    "from typing import TypedDict, Annotated, List\n",
    "\n",
    "# Create powermem instance\n",
    "config = auto_config()\n",
    "powermem = Memory(config=config)\n",
    "\n",
    "# Define state schema\n",
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], \"Conversation messages\"]\n",
    "    user_id: str\n",
    "    context: dict\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Define nodes\n",
    "def load_context(state: ConversationState) -> ConversationState:\n",
    "    \"\"\"Load relevant context from powermem.\"\"\"\n",
    "    user_id = state[\"user_id\"]\n",
    "    last_message = state[\"messages\"][-1] if state[\"messages\"] else None\n",
    "    query = last_message.content if last_message else \"\"\n",
    "    \n",
    "    # Search powermem\n",
    "    results = powermem.search(query=query, user_id=user_id, limit=5)\n",
    "    state[\"context\"] = {\n",
    "        \"memories\": [mem.get('memory', '') for mem in results.get('results', [])]\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def generate_response(state: ConversationState) -> ConversationState:\n",
    "    \"\"\"Generate response using LLM.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    context_str = \"\\n\".join(state[\"context\"].get(\"memories\", []))\n",
    "    \n",
    "    prompt = f\"\"\"Context from previous conversations:\n",
    "{context_str}\n",
    "\n",
    "User: {last_message.content}\n",
    "Assistant:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    state[\"messages\"].append(AIMessage(content=response.content))\n",
    "    return state\n",
    "\n",
    "def save_conversation(state: ConversationState) -> ConversationState:\n",
    "    \"\"\"Save conversation to powermem.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) >= 2:\n",
    "        user_msg = messages[-2]\n",
    "        ai_msg = messages[-1]\n",
    "        \n",
    "        powermem.add(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_msg.content},\n",
    "                {\"role\": \"assistant\", \"content\": ai_msg.content}\n",
    "            ],\n",
    "            user_id=state[\"user_id\"],\n",
    "            infer=True\n",
    "        )\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(ConversationState)\n",
    "workflow.add_node(\"load_context\", load_context)\n",
    "workflow.add_node(\"generate_response\", generate_response)\n",
    "workflow.add_node(\"save_conversation\", save_conversation)\n",
    "\n",
    "workflow.add_edge(START, \"load_context\")\n",
    "workflow.add_edge(\"load_context\", \"generate_response\")\n",
    "workflow.add_edge(\"generate_response\", \"save_conversation\")\n",
    "workflow.add_edge(\"save_conversation\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Use the graph\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Hello, I'm Alice\")],\n",
    "    \"user_id\": \"user123\",\n",
    "    \"context\": {}\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "print(final_state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: FastAPI Integration\n",
    "\n",
    "Create a FastAPI application with powermem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastapi_integration.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from powermem import AsyncMemory, auto_config\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "config = auto_config()\n",
    "async_memory = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    global async_memory\n",
    "    async_memory = AsyncMemory(config=config)\n",
    "    await async_memory.initialize()\n",
    "\n",
    "class MemoryRequest(BaseModel):\n",
    "    memory: str\n",
    "    user_id: str\n",
    "    metadata: dict = {}\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "    user_id: str\n",
    "    limit: int = 10\n",
    "\n",
    "@app.post(\"/memories\")\n",
    "async def add_memory(request: MemoryRequest):\n",
    "    \"\"\"Add a memory\"\"\"\n",
    "    try:\n",
    "        result = await async_memory.add(\n",
    "            memory=request.memory,\n",
    "            user_id=request.user_id,\n",
    "            metadata=request.metadata\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/memories/search\")\n",
    "async def search_memories(request: SearchRequest):\n",
    "    \"\"\"Search memories\"\"\"\n",
    "    try:\n",
    "        results = await async_memory.search(\n",
    "            query=request.query,\n",
    "            user_id=request.user_id,\n",
    "            limit=request.limit\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/memories/{user_id}\")\n",
    "async def get_all_memories(user_id: str):\n",
    "    \"\"\"Get all memories for a user\"\"\"\n",
    "    try:\n",
    "        results = await async_memory.get_all(user_id=user_id)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.delete(\"/memories/{user_id}\")\n",
    "async def delete_all_memories(user_id: str):\n",
    "    \"\"\"Delete all memories for a user\"\"\"\n",
    "    try:\n",
    "        result = await async_memory.delete_all(user_id=user_id)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Run with: uvicorn fastapi_integration:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Custom Intelligence Plugin\n",
    "\n",
    "Implement a custom intelligence plugin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_intelligence_plugin.py\n",
    "from powermem.intelligence.plugin import IntelligentMemoryPlugin\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class CustomIntelligencePlugin(IntelligentMemoryPlugin):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.decay_rate = config.get('decay_rate', 0.1)\n",
    "    \n",
    "    def calculate_retention(self, memory: Dict[str, Any], \n",
    "                           time_since_creation: float) -> float:\n",
    "        \"\"\"Custom retention calculation\"\"\"\n",
    "        # Your custom retention logic\n",
    "        initial_retention = 1.0\n",
    "        retention = initial_retention * (0.9 ** (time_since_creation / 86400))\n",
    "        return max(0.0, min(1.0, retention))\n",
    "    \n",
    "    def score_importance(self, memory: Dict[str, Any]) -> float:\n",
    "        \"\"\"Custom importance scoring\"\"\"\n",
    "        # Your custom importance logic\n",
    "        content = memory.get('memory', '')\n",
    "        if 'important' in content.lower() or 'critical' in content.lower():\n",
    "            return 0.9\n",
    "        return 0.5\n",
    "\n",
    "# Use custom plugin\n",
    "config = {\n",
    "    'intelligent_memory': {\n",
    "        'enabled': True,\n",
    "        'plugin': 'custom',\n",
    "        'decay_rate': 0.1\n",
    "    },\n",
    "    # ... other config\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example\n",
    "\n",
    "Here's a complete custom integration example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_custom_integration.py\n",
    "from powermem import Memory\n",
    "from powermem.integrations.llm.base import LLMBase\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class SimpleLLM(LLMBase):\n",
    "    def __init__(self, config):\n",
    "        self.model = config.get('model', 'simple')\n",
    "    \n",
    "    def generate(self, prompt, **kwargs):\n",
    "        return f\"Response: {prompt}\"\n",
    "    \n",
    "    def extract_facts(self, messages):\n",
    "        facts = []\n",
    "        for msg in messages:\n",
    "            if msg.get('role') == 'user':\n",
    "                content = msg.get('content', '')\n",
    "                if content:\n",
    "                    facts.append(content)\n",
    "        return facts\n",
    "\n",
    "# Register\n",
    "from powermem.integrations.llm.factory import LLMFactory\n",
    "LLMFactory.register('simple', SimpleLLM)\n",
    "\n",
    "# Use\n",
    "config = {\n",
    "    'llm': {\n",
    "        'provider': 'simple',\n",
    "        'config': {'model': 'simple'}\n",
    "    },\n",
    "    'embedder': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'text-embedding-v4'}\n",
    "    },\n",
    "    'vector_store': {\n",
    "        'provider': 'sqlite',\n",
    "        'config': {'path': './memories.db'}\n",
    "    }\n",
    "}\n",
    "\n",
    "memory = Memory(config=config)\n",
    "result = memory.add(messages=\"Test memory\", user_id=\"user123\")\n",
    "print(f\"âœ“ Added memory with custom LLM: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extension Exercises\n",
    "\n",
    "### Exercise 1: Custom Storage Backend\n",
    "\n",
    "Implement a file-based storage backend:\n",
    "\n",
    "```python\n",
    "class FileVectorStore(VectorStoreBase):\n",
    "    def __init__(self, config):\n",
    "        self.file_path = config.get('file_path', './memories.json')\n",
    "        self.memories = self._load()\n",
    "    \n",
    "    def _load(self):\n",
    "        import json\n",
    "        import os\n",
    "        if os.path.exists(self.file_path):\n",
    "            with open(self.file_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save(self):\n",
    "        import json\n",
    "        with open(self.file_path, 'w') as f:\n",
    "            json.dump(self.memories, f)\n",
    "    \n",
    "    def add(self, memory, embedding, metadata):\n",
    "        memory_id = f\"mem_{len(self.memories)}\"\n",
    "        self.memories[memory_id] = {\n",
    "            'memory': memory,\n",
    "            'embedding': embedding,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        self._save()\n",
    "        return memory_id\n",
    "```\n",
    "\n",
    "### Exercise 2: Custom Embedding with Caching\n",
    "\n",
    "Add caching to custom embedder:\n",
    "\n",
    "```python\n",
    "class CachedEmbedder(EmbedderBase):\n",
    "    def __init__(self, config):\n",
    "        self.cache = {}\n",
    "        self.base_embedder = YourEmbedder(config)\n",
    "    \n",
    "    def embed(self, text):\n",
    "        if text in self.cache:\n",
    "            return self.cache[text]\n",
    "        embedding = self.base_embedder.embed(text)\n",
    "        self.cache[text] = embedding\n",
    "        return embedding\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Follow interfaces**: Implement required methods from base classes\n",
    "2. **Error handling**: Handle errors gracefully\n",
    "3. **Configuration**: Use configuration dictionaries for flexibility\n",
    "4. **Testing**: Test custom providers thoroughly\n",
    "5. **Documentation**: Document custom implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **API Reference**: See [API Documentation](../api/)\n",
    "- **Architecture**: Understand [System Architecture](../architecture/)\n",
    "- **Guides**: Check [Integration Guide](../guides/integrations.md)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
