{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 5: Custom Integration\n",
    "\n",
    "This scenario demonstrates how to integrate powermem with custom systems, implement custom providers, and extend functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Completed previous scenarios\n",
    "- Understanding of Python classes and interfaces\n",
    "- Basic knowledge of LLM/embedding providers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Custom Integration\n",
    "\n",
    "powermem is designed to be extensible:\n",
    "- Custom LLM providers\n",
    "- Custom embedding providers\n",
    "- Custom storage backends\n",
    "- Custom intelligence plugins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Provider and Factory Configuration\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "from powermem.integrations.llm.config.base import BaseLLMConfig\n",
    "from powermem.integrations.embeddings.config.base import BaseEmbedderConfig\n",
    "from powermem.storage.config.base import BaseVectorStoreConfig\n",
    "\n",
    "class CustomLLMConfig(BaseLLMConfig):\n",
    "    base_url: str | None = Field(default=None)\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "\n",
    "class CustomEmbedderConfig(BaseEmbedderConfig):\n",
    "    dims: int = Field(default=768)\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "\n",
    "class CustomVectorStoreConfig(BaseVectorStoreConfig):\n",
    "    connection_string: str = Field(default='')\n",
    "    collection_name: str = Field(default='memories')\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "\n",
    "from powermem.integrations.llm.factory import LLMFactory\n",
    "from powermem.integrations.embeddings.factory import EmbedderFactory\n",
    "from powermem.storage.factory import VectorStoreFactory\n",
    "\n",
    "LLMFactory.provider_to_class.update({\n",
    "    \"custom\": (\"powermem.integrations.llm.custom.CustomLLM\", CustomLLMConfig),\n",
    "})\n",
    "\n",
    "EmbedderFactory.provider_to_class.update({\n",
    "    \"custom\": \"powermem.integrations.embeddings.custom.CustomEmbedder\"\n",
    "})\n",
    "\n",
    "VectorStoreFactory.provider_to_class.update({\n",
    "     \"custom\": \"powermem.storage.custom.custom_integration_example.CustomVectorStore\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Custom LLM Provider\n",
    "\n",
    "Implement a custom LLM provider:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from powermem.integrations.llm.base import LLMBase\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class CustomLLM(LLMBase):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.api_key = config.get('api_key', '')\n",
    "        self.model = config.get('model', 'default')\n",
    "        self.base_url = config.get('base_url', 'https://api.example.com')\n",
    "    \n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Generate text using custom LLM\"\"\"\n",
    "        # Your custom LLM implementation\n",
    "        # This is a mock example\n",
    "        return f\"Response to: {prompt}\"\n",
    "    \n",
    "    def extract_facts(self, messages: List[Dict[str, str]]) -> List[str]:\n",
    "        \"\"\"Extract facts from messages\"\"\"\n",
    "        # Custom fact extraction logic\n",
    "        facts = []\n",
    "        for msg in messages:\n",
    "            if msg.get('role') == 'user':\n",
    "                content = msg.get('content', '')\n",
    "                # Simple extraction (replace with your logic)\n",
    "                if 'name is' in content.lower():\n",
    "                    facts.append(f\"Name: {content.split('name is')[1].strip()}\")\n",
    "        return facts\n",
    "\n",
    "# Register custom LLM\n",
    "from powermem.integrations.llm.factory import LLMFactory\n",
    "\n",
    "# Register your custom provider\n",
    "LLMFactory.register('custom', CustomLLM)\n",
    "\n",
    "# Use custom LLM\n",
    "config = {\n",
    "    'llm': {\n",
    "        'provider': 'custom',\n",
    "        'config': {\n",
    "            'api_key': 'your_key',\n",
    "            'model': 'your_model',\n",
    "            'base_url': 'https://api.example.com'\n",
    "        }\n",
    "    },\n",
    "    'embedder': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'text-embedding-v4'}\n",
    "    },\n",
    "    'vector_store': {\n",
    "        'provider': 'sqlite',\n",
    "        'config': {'path': './memories.db'}\n",
    "    }\n",
    "}\n",
    "\n",
    "from powermem import Memory\n",
    "memory = Memory(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Custom Embedding Provider\n",
    "\n",
    "Implement a custom embedding provider:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from powermem.integrations.embeddings.base import EmbedderBase\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "class CustomEmbedder(EmbedderBase):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.api_key = config.get('api_key', '')\n",
    "        self.model = config.get('model', 'default')\n",
    "        self.dims = config.get('dims', 768)\n",
    "    \n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for text\"\"\"\n",
    "        # Your custom embedding implementation\n",
    "        # This is a mock example\n",
    "        # In real implementation, call your embedding API\n",
    "        return np.random.rand(self.dims).tolist()\n",
    "    \n",
    "    def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for batch of texts\"\"\"\n",
    "        return [self.embed(text) for text in texts]\n",
    "\n",
    "# Register custom embedder\n",
    "from powermem.integrations.embeddings.factory import EmbedderFactory\n",
    "\n",
    "EmbedderFactory.register('custom', CustomEmbedder)\n",
    "\n",
    "# Use custom embedder\n",
    "config = {\n",
    "    'llm': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'qwen-plus'}\n",
    "    },\n",
    "    'embedder': {\n",
    "        'provider': 'custom',\n",
    "        'config': {\n",
    "            'api_key': 'your_key',\n",
    "            'model': 'your_model',\n",
    "            'dims': 768\n",
    "        }\n",
    "    },\n",
    "    'vector_store': {\n",
    "        'provider': 'sqlite',\n",
    "        'config': {'path': './memories.db'}\n",
    "    }\n",
    "}\n",
    "\n",
    "memory = Memory(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Custom Storage Backend\n",
    "\n",
    "Implement a custom storage backend:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from powermem.storage.base import VectorStoreBase\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class CustomVectorStore(VectorStoreBase):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.connection_string = config.get('connection_string', '')\n",
    "        self.collection_name = config.get('collection_name', 'memories')\n",
    "        # Initialize your custom storage connection\n",
    "        self._init_connection()\n",
    "    \n",
    "    def _init_connection(self):\n",
    "        \"\"\"Initialize connection to your storage\"\"\"\n",
    "        # Your connection initialization logic\n",
    "        pass\n",
    "    \n",
    "    def add(self, memory: str, embedding: List[float], metadata: Dict[str, Any]) -> str:\n",
    "        \"\"\"Add memory to storage\"\"\"\n",
    "        memory_id = f\"mem_{hash(memory)}\"\n",
    "        # Your storage implementation\n",
    "        # Store memory, embedding, and metadata\n",
    "        return memory_id\n",
    "    \n",
    "    def search(self, query_embedding: List[float], limit: int = 10, \n",
    "               filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search similar memories\"\"\"\n",
    "        # Your search implementation\n",
    "        # Return list of memories with scores\n",
    "        return []\n",
    "    \n",
    "    def get(self, memory_id: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get memory by ID\"\"\"\n",
    "        # Your retrieval implementation\n",
    "        return None\n",
    "    \n",
    "    def update(self, memory_id: str, memory: Optional[str] = None,\n",
    "               metadata: Optional[Dict[str, Any]] = None) -> bool:\n",
    "        \"\"\"Update memory\"\"\"\n",
    "        # Your update implementation\n",
    "        return True\n",
    "    \n",
    "    def delete(self, memory_id: str) -> bool:\n",
    "        \"\"\"Delete memory\"\"\"\n",
    "        # Your deletion implementation\n",
    "        return True\n",
    "    \n",
    "    def delete_all(self, filters: Optional[Dict[str, Any]] = None) -> int:\n",
    "        \"\"\"Delete all matching memories\"\"\"\n",
    "        # Your bulk deletion implementation\n",
    "        return 0\n",
    "\n",
    "# Register custom storage\n",
    "from powermem.storage.factory import VectorStoreFactory\n",
    "\n",
    "VectorStoreFactory.register('custom', CustomVectorStore)\n",
    "\n",
    "# Use custom storage\n",
    "config = {\n",
    "    'llm': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'qwen-plus'}\n",
    "    },\n",
    "    'embedder': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'text-embedding-v4'}\n",
    "    },\n",
    "    'vector_store': {\n",
    "        'provider': 'custom',\n",
    "        'config': {\n",
    "            'connection_string': 'your_connection_string',\n",
    "            'collection_name': 'memories'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "memory = Memory(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: LangChain Integration\n",
    "\n",
    "Integrate powermem with LangChain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_integration.py\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from powermem import Memory, auto_config\n",
    "\n",
    "class PowermemLangChainMemory(ConversationBufferMemory):\n",
    "    def __init__(self, powermem_instance: Memory, user_id: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.powermem = powermem_instance\n",
    "        self.user_id = user_id\n",
    "    \n",
    "    def save_context(self, inputs, outputs):\n",
    "        \"\"\"Save conversation to powermem\"\"\"\n",
    "        super().save_context(inputs, outputs)\n",
    "        \n",
    "        # Convert to messages format\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": str(inputs)},\n",
    "            {\"role\": \"assistant\", \"content\": str(outputs)}\n",
    "        ]\n",
    "        \n",
    "        # Save to powermem with intelligent processing\n",
    "        self.powermem.add(\n",
    "            messages=messages,\n",
    "            user_id=self.user_id,\n",
    "            infer=True\n",
    "        )\n",
    "    \n",
    "    def load_memory_variables(self, inputs):\n",
    "        \"\"\"Load relevant memories from powermem\"\"\"\n",
    "        query = str(inputs)\n",
    "        \n",
    "        # Search powermem for relevant memories\n",
    "        results = self.powermem.search(\n",
    "            query=query,\n",
    "            user_id=self.user_id,\n",
    "            limit=5\n",
    "        )\n",
    "        \n",
    "        # Format for LangChain\n",
    "        memory_variables = {\n",
    "            \"history\": \"\\n\".join([\n",
    "                result['memory'] for result in results.get('results', [])\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        return memory_variables\n",
    "\n",
    "# Usage with LangChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "config = auto_config()\n",
    "powermem = Memory(config=config)\n",
    "memory = PowermemLangChainMemory(powermem, user_id=\"user123\")\n",
    "\n",
    "llm = OpenAI()\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "response = chain.run(\"Hello, I'm Alice\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: FastAPI Integration\n",
    "\n",
    "Create a FastAPI application with powermem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastapi_integration.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from powermem import AsyncMemory, auto_config\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "config = auto_config()\n",
    "async_memory = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    global async_memory\n",
    "    async_memory = AsyncMemory(config=config)\n",
    "    await async_memory.initialize()\n",
    "\n",
    "class MemoryRequest(BaseModel):\n",
    "    memory: str\n",
    "    user_id: str\n",
    "    metadata: dict = {}\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "    user_id: str\n",
    "    limit: int = 10\n",
    "\n",
    "@app.post(\"/memories\")\n",
    "async def add_memory(request: MemoryRequest):\n",
    "    \"\"\"Add a memory\"\"\"\n",
    "    try:\n",
    "        result = await async_memory.add(\n",
    "            memory=request.memory,\n",
    "            user_id=request.user_id,\n",
    "            metadata=request.metadata\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/memories/search\")\n",
    "async def search_memories(request: SearchRequest):\n",
    "    \"\"\"Search memories\"\"\"\n",
    "    try:\n",
    "        results = await async_memory.search(\n",
    "            query=request.query,\n",
    "            user_id=request.user_id,\n",
    "            limit=request.limit\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/memories/{user_id}\")\n",
    "async def get_all_memories(user_id: str):\n",
    "    \"\"\"Get all memories for a user\"\"\"\n",
    "    try:\n",
    "        results = await async_memory.get_all(user_id=user_id)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.delete(\"/memories/{user_id}\")\n",
    "async def delete_all_memories(user_id: str):\n",
    "    \"\"\"Delete all memories for a user\"\"\"\n",
    "    try:\n",
    "        result = await async_memory.delete_all(user_id=user_id)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Run with: uvicorn fastapi_integration:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Custom Intelligence Plugin\n",
    "\n",
    "Implement a custom intelligence plugin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_intelligence_plugin.py\n",
    "from powermem.intelligence.plugin import IntelligentMemoryPlugin\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class CustomIntelligencePlugin(IntelligentMemoryPlugin):\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.decay_rate = config.get('decay_rate', 0.1)\n",
    "    \n",
    "    def calculate_retention(self, memory: Dict[str, Any], \n",
    "                           time_since_creation: float) -> float:\n",
    "        \"\"\"Custom retention calculation\"\"\"\n",
    "        # Your custom retention logic\n",
    "        initial_retention = 1.0\n",
    "        retention = initial_retention * (0.9 ** (time_since_creation / 86400))\n",
    "        return max(0.0, min(1.0, retention))\n",
    "    \n",
    "    def score_importance(self, memory: Dict[str, Any]) -> float:\n",
    "        \"\"\"Custom importance scoring\"\"\"\n",
    "        # Your custom importance logic\n",
    "        content = memory.get('memory', '')\n",
    "        if 'important' in content.lower() or 'critical' in content.lower():\n",
    "            return 0.9\n",
    "        return 0.5\n",
    "\n",
    "# Use custom plugin\n",
    "config = {\n",
    "    'intelligent_memory': {\n",
    "        'enabled': True,\n",
    "        'plugin': 'custom',\n",
    "        'decay_rate': 0.1\n",
    "    },\n",
    "    # ... other config\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example\n",
    "\n",
    "Here's a complete custom integration example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_custom_integration.py\n",
    "from powermem import Memory\n",
    "from powermem.integrations.llm.base import LLMBase\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class SimpleLLM(LLMBase):\n",
    "    def __init__(self, config):\n",
    "        self.model = config.get('model', 'simple')\n",
    "    \n",
    "    def generate(self, prompt, **kwargs):\n",
    "        return f\"Response: {prompt}\"\n",
    "    \n",
    "    def extract_facts(self, messages):\n",
    "        facts = []\n",
    "        for msg in messages:\n",
    "            if msg.get('role') == 'user':\n",
    "                content = msg.get('content', '')\n",
    "                if content:\n",
    "                    facts.append(content)\n",
    "        return facts\n",
    "\n",
    "# Register\n",
    "from powermem.integrations.llm.factory import LLMFactory\n",
    "LLMFactory.register('simple', SimpleLLM)\n",
    "\n",
    "# Use\n",
    "config = {\n",
    "    'llm': {\n",
    "        'provider': 'simple',\n",
    "        'config': {'model': 'simple'}\n",
    "    },\n",
    "    'embedder': {\n",
    "        'provider': 'qwen',\n",
    "        'config': {'api_key': 'your_key', 'model': 'text-embedding-v4'}\n",
    "    },\n",
    "    'vector_store': {\n",
    "        'provider': 'sqlite',\n",
    "        'config': {'path': './memories.db'}\n",
    "    }\n",
    "}\n",
    "\n",
    "memory = Memory(config=config)\n",
    "result = memory.add(\"Test memory\", user_id=\"user123\")\n",
    "print(f\"âœ“ Added memory with custom LLM: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Exercises\n",
    "\n",
    "### Exercise 1: Custom Storage Backend\n",
    "\n",
    "Implement a file-based storage backend:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileVectorStore(VectorStoreBase):\n",
    "    def __init__(self, config):\n",
    "        self.file_path = config.get('file_path', './memories.json')\n",
    "        self.memories = self._load()\n",
    "    \n",
    "    def _load(self):\n",
    "        import json\n",
    "        import os\n",
    "        if os.path.exists(self.file_path):\n",
    "            with open(self.file_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save(self):\n",
    "        import json\n",
    "        with open(self.file_path, 'w') as f:\n",
    "            json.dump(self.memories, f)\n",
    "    \n",
    "    def add(self, memory, embedding, metadata):\n",
    "        memory_id = f\"mem_{len(self.memories)}\"\n",
    "        self.memories[memory_id] = {\n",
    "            'memory': memory,\n",
    "            'embedding': embedding,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        self._save()\n",
    "        return memory_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedEmbedder(EmbedderBase):\n",
    "    def __init__(self, config):\n",
    "        self.cache = {}\n",
    "        self.base_embedder = YourEmbedder(config)\n",
    "    \n",
    "    def embed(self, text):\n",
    "        if text in self.cache:\n",
    "            return self.cache[text]\n",
    "        embedding = self.base_embedder.embed(text)\n",
    "        self.cache[text] = embedding\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Follow interfaces**: Implement required methods from base classes\n",
    "2. **Error handling**: Handle errors gracefully\n",
    "3. **Configuration**: Use configuration dictionaries for flexibility\n",
    "4. **Testing**: Test custom providers thoroughly\n",
    "5. **Documentation**: Document custom implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **API Reference**: See [API Documentation](../api/)\n",
    "- **Architecture**: Understand [System Architecture](../architecture/)\n",
    "- **Guides**: Check [Integration Guide](../guides/integrations.md)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
