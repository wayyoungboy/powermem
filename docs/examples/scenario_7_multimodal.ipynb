{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4368028b",
   "metadata": {},
   "source": [
    "# Scenario 7: Multimodal Capability\n",
    "\n",
    "This scenario demonstrates PowerMem's multimodal capability - storing and retrieving images, audio, and other multimedia content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1187ba",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- powermem installed (`pip install powermem`)\n",
    "- Multimodal LLM API support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db33b2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Create `.env` file and configure multimodal parameters:\n",
    "```bash\n",
    "cp .env.example .env\n",
    "```\n",
    "\n",
    "> **Note:** Multimodal functionality requires vision-capable LLM models such as `gpt-4o`, `gpt-4-vision-preview`, or `qwen-vl-plus`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c244189",
   "metadata": {},
   "source": [
    "## What is Multimodal Capability?\n",
    "\n",
    "Multimodal capability allows PowerMem to process more than just text:\n",
    "- **Images**: Extract information from images and generate text descriptions\n",
    "- **Image URLs**: Process online image links\n",
    "- **Audio**: Process audio files and convert speech to text\n",
    "- **Audio URLs**: Process online audio links\n",
    "- **Mixed Content**: Handle composite messages with both text, images, and audio\n",
    "\n",
    "PowerMem automatically converts image and audio content to text descriptions, stores them as memories, making the multimedia content searchable and retrievable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a8e87",
   "metadata": {},
   "source": [
    "## Step 1: Add Image Memory Using OpenAI Multimodal Format\n",
    "\n",
    "Add image-containing memories using the standard OpenAI multimodal message format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08694ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from powermem import Memory\n",
    "\n",
    "image_url = \"https://example.com/workspace.jpg\"\n",
    "\n",
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"openai\",  # Use OpenAI-compatible multimodal model\n",
    "        \"config\": {\n",
    "            \"model\": \"qwen-vl-plus\",\n",
    "            \"enable_vision\": True,  # Key: Enable vision processing\n",
    "            \"vision_details\": \"auto\",  # Image analysis precision: auto/low/high\n",
    "            \"api_key\": \"your-api-key\",\n",
    "            \"openai_base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        }\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"oceanbase\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"simple_test\",\n",
    "            \"embedding_model_dims\": 1536,\n",
    "            \"host\": os.getenv(\"OCEANBASE_HOST\", \"127.0.0.1\"),\n",
    "            \"port\": int(os.getenv(\"OCEANBASE_PORT\", \"2881\")),\n",
    "            \"user\": os.getenv(\"OCEANBASE_USER\", \"root@sys\"),\n",
    "            \"password\": os.getenv(\"OCEANBASE_PASSWORD\", \"\"),\n",
    "            \"db_name\": os.getenv(\"OCEANBASE_DB\", \"test\"),\n",
    "        }\n",
    "    },\n",
    "    \"version\": \"v1.1\",\n",
    "    \"embedding\": {\n",
    "        \"provider\": \"qwen\",\n",
    "        \"config\": {\n",
    "            \"model\": \"text-embedding-v4\",\n",
    "            \"embedding_dims\": 1536,\n",
    "            \"api_key\": \"your-api-key\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "memory = Memory(config=config)\n",
    "\n",
    "# OpenAI multimodal message format\n",
    "messages_multimodal = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"This is Bob's favorite working state\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": image_url,\n",
    "                    \"detail\": \"auto\"  # Optional: auto/low/high\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add image memory\n",
    "result = memory.add(\n",
    "    messages=messages_multimodal,\n",
    "    user_id=\"test_user\",\n",
    "    metadata={\"type\": \"workspace_photo\", \"source\": \"user_upload\"}\n",
    ")\n",
    "\n",
    "print(f\"✅ Successfully added image memory\")\n",
    "print(f\"   Memory ID: {result.get('id')}\")\n",
    "print(f\"   Processed content: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110305c",
   "metadata": {},
   "source": [
    "## Step 2: Search Image-Related Memories\n",
    "\n",
    "Search for previously added image memories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c40222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for image-related memories\n",
    "query = \"Bob's favorite working state\"\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "results = memory.search(query=query, user_id=\"test_user\", limit=3)\n",
    "\n",
    "if results.get(\"results\"):\n",
    "    for idx, mem in enumerate(results[\"results\"], 1):\n",
    "        print(f\"  Result {idx}: {mem.get('memory', '')[:100]}...\")\n",
    "        print(f\"    Similarity: {mem.get('score', 0):.4f}\")\n",
    "        print(f\"    Metadata: {mem.get('metadata', {})}\")\n",
    "else:\n",
    "    print(\"  No related memories found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb465b",
   "metadata": {},
   "source": [
    "## Step 3: Add Audio Memory Using OpenAI Multimodal Format\n",
    "\n",
    "Add audio-containing memories using the standard OpenAI multimodal message format. **Important: Audio must be provided as a URL, just like images.**\n",
    "\n",
    "> **Note:** Audio processing requires `enable_vision: True` to be set in the LLM configuration, even though it's audio content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from powermem import Memory\n",
    "\n",
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"openai\",  # Use OpenAI-compatible multimodal model\n",
    "        \"config\": {\n",
    "            \"model\": \"qwen-vl-plus\",\n",
    "            \"enable_vision\": True,  # Required: Must be True for audio processing\n",
    "            \"vision_details\": \"auto\",\n",
    "            \"api_key\": \"your-api-key\",\n",
    "            \"openai_base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        }\n",
    "    },\n",
    "    \"audio_llm\": {\n",
    "        \"provider\": \"qwen_asr\",\n",
    "        \"config\": {\n",
    "            \"model\": \"qwen3-asr-flash\",\n",
    "            \"api_key\": \"your-api-key\",\n",
    "        }\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"oceanbase\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"simple_test\",\n",
    "            \"embedding_model_dims\": 1536,\n",
    "            \"host\": os.getenv(\"OCEANBASE_HOST\", \"127.0.0.1\"),\n",
    "            \"port\": int(os.getenv(\"OCEANBASE_PORT\", \"2881\")),\n",
    "            \"user\": os.getenv(\"OCEANBASE_USER\", \"root@sys\"),\n",
    "            \"password\": os.getenv(\"OCEANBASE_PASSWORD\", \"\"),\n",
    "            \"db_name\": os.getenv(\"OCEANBASE_DB\", \"test\"),\n",
    "        }\n",
    "    },\n",
    "    \"version\": \"v1.1\",\n",
    "    \"embedding\": {\n",
    "        \"provider\": \"qwen\",\n",
    "        \"config\": {\n",
    "            \"model\": \"text-embedding-v4\",\n",
    "            \"embedding_dims\": 1536,\n",
    "            \"api_key\": \"your-api-key\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "memory = Memory(config=config)\n",
    "\n",
    "# Audio URL (must be a URL, not a local file path)\n",
    "audio_url = \"https://example.com/example.wav\"\n",
    "\n",
    "# OpenAI multimodal message format with audio\n",
    "messages_multimodal = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"This is a voice message from Alice\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"audio\",\n",
    "                \"content\": {\n",
    "                    \"audio\": audio_url,  # Must be a URL\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add audio memory\n",
    "result = memory.add(\n",
    "    messages=messages_multimodal,\n",
    "    user_id=\"test_user\",\n",
    "    metadata={\"type\": \"voice_message\", \"source\": \"user_upload\"}\n",
    ")\n",
    "\n",
    "print(f\"✅ Successfully added audio memory\")\n",
    "print(f\"   Memory ID: {result.get('id')}\")\n",
    "print(f\"   Processed content: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a43151",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "- Audio must be provided as a **URL** (not a local file path)\n",
    "- `enable_vision: True` must be set in LLM config (required for audio processing)\n",
    "- `audio_llm` configuration is required for audio transcription\n",
    "- Audio content is automatically converted to text and stored as searchable memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d068f",
   "metadata": {},
   "source": [
    "## Step 4: Search Audio-Related Memories\n",
    "\n",
    "Search for previously added audio memories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb17283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for audio-related memories\n",
    "query = \"voice message from Alice\"\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "results = memory.search(query=query, user_id=\"test_user\", limit=3)\n",
    "\n",
    "if results.get(\"results\"):\n",
    "    for idx, mem in enumerate(results[\"results\"], 1):\n",
    "        print(f\"  Result {idx}: {mem.get('memory', '')[:100]}...\")\n",
    "        print(f\"    Similarity: {mem.get('score', 0):.4f}\")\n",
    "        print(f\"    Metadata: {mem.get('metadata', {})}\")\n",
    "else:\n",
    "    print(\"  No related memories found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf264c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Scenario 2**: Learn about intelligent memory features\n",
    "- **Scenario 4**: Explore async operations\n",
    "- **API Reference**: See [Memory API](../api/0001-memory.md)\n",
    "- **Configuration Guide**: See [Configuration Guide](../guides/0002-configuration.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
